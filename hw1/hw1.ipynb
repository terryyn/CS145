{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW1 is due on **11:59 PM PT, Oct 19 (Monday, Week 3)**. Please submit through GradeScope (you will receive an invite to Gradescope for CS145 Fall 2020.). \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Terry Ye, UID: 004757414** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment by the given `cs145hw1.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw1.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This workbook will walk you through a linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(lm, beta):\n",
    "    train_predict_y = lm.predict(lm.train_x, beta)\n",
    "    test_predict_y = lm.predict(lm.test_x, beta)\n",
    "    training_error = lm.compute_mse(train_predict_y, lm.train_y)\n",
    "    testing_error = lm.compute_mse(test_predict_y, lm.test_y)\n",
    "    return training_error, testing_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py` which use the close for solution of $\\hat{\\beta}$.\n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675803\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "training_error, testing_error = train_and_predict(lm, beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py` which compute the gradient of the objective fuction. \n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training accuracy is:  0.08693895533150824\n",
      "Testing accuracy is:  0.11016592170824556\n"
     ]
    }
   ],
   "source": [
    "\"\"\"add import here so can just run this cell alone when update the source code\"\"\"\n",
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('1')\n",
    "training_error, testing_error = train_and_predict(lm, beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which use an estimated gradient of the objective function.\n",
    "\n",
    "Train you model by using `lm.train('2')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  2\n",
      "Training accuracy is:  0.09337211730536127\n",
      "Testing accuracy is:  0.11828923255919811\n"
     ]
    }
   ],
   "source": [
    "\"\"\"add import here so can just run this cell alone when update the source code\"\"\"\n",
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('2')\n",
    "training_error, testing_error = train_and_predict(lm, beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error for 0 =  0.08693886675396784\n",
      "Testing error for 0 =  0.11017540281675804\n",
      "Learning Algorithm Type:  1\n",
      "Training error for 1 =  0.10024222412685467\n",
      "Testing error for 1 =  0.14039955364152018\n",
      "Learning Algorithm Type:  2\n",
      "Training error for 2 =  0.0869952008695027\n",
      "Testing error for 2 =  0.1099647262725722\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Normalize data\"\"\"\n",
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "lm.normalize()\n",
    "for train_type in ['0', '1', '2']:\n",
    "    beta = lm.train(train_type)\n",
    "    training_error, testing_error = train_and_predict(lm, beta)\n",
    "    print('Training error for ' + train_type + ' = ', training_error )\n",
    "    print('Testing error for ' + train_type + ' = ', testing_error )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the MSE on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Apply z-score normalization for eachh featrure and comment whether or not it affect the three algorithm. \n",
    "3. Ridge regression is adding an L2 regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "where $\\lambda \\leq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> \n",
    "    1. The testing errors are not the same and slightly different. The Closed Form and Batch Gradient performed similarly while the Stochastic Graident performed a bit worse. I think Closed Form and Batch Gradient all reached the optimum and the little difference is because Batch Gradient has learning rate which prevents it from achieving the absolute best in limited iterations. The Stochastic Graidient performed worse maybe because the learning rate is smaller and it converges much slower.<br>\n",
    "    2. It does not affect the Closed Form algorithm but affects Batch Gradient and Stochastic Gradient algorithms. <br>\n",
    "    3. The matrix form of objective function can be written as \n",
    "        $ \\frac{1}{2n}(X\\beta - y)^T(X\\beta - y) + \\frac{\\lambda}{2n}\\beta^T\\beta $ <br>\n",
    "       Taking derivative of the matrix form as 0:\n",
    "        $ J(\\beta)' = (X^TX\\beta - X^Ty)/n + \\frac{\\lambda}{n}\\beta = 0$ <br>\n",
    "       So $(X^TX + \\lambda)\\beta = X^Ty$, the closed form solution of $\\beta$ is \n",
    "        $$ \\beta = ï¼ˆX^TX + \\lambda I)^{-1}X^Ty$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This workbook will walk you through a logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_logistic(lm, beta):\n",
    "    train_predict_y = lm.predict(lm.train_x, beta)\n",
    "    test_predict_y = lm.predict(lm.test_x, beta)\n",
    "    training_accuracy = lm.compute_accuracy(train_predict_y, lm.train_y)\n",
    "    testing_accuracy = lm.compute_accuracy(test_predict_y, lm.test_y)\n",
    "    return training_accuracy, testing_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which compute the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check. \n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "And print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.48427563333314566 \t\n",
      "average logL for iteration 1000: -0.46010037535085324 \t\n",
      "average logL for iteration 2000: -0.46010037535085324 \t\n",
      "average logL for iteration 3000: -0.46010037535085324 \t\n",
      "average logL for iteration 4000: -0.46010037535085324 \t\n",
      "average logL for iteration 5000: -0.46010037535085324 \t\n",
      "average logL for iteration 6000: -0.46010037535085324 \t\n",
      "average logL for iteration 7000: -0.46010037535085324 \t\n",
      "average logL for iteration 8000: -0.46010037535085324 \t\n",
      "average logL for iteration 9000: -0.46010037535085324 \t\n",
      "Training avgLogL:  -0.46010037535085324\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "\"\"\"add import here so can just run this cell alone when update the source code\"\"\"\n",
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('0')\n",
    "training_accuracy, testing_accuracy = train_and_predict_logistic(lm, beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` in `logistic_regression.py`, which make use of both first and second derivative.\n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.5564001859888966 \t\n",
      "average logL for iteration 500: -0.46010037535085324 \t\n",
      "average logL for iteration 1000: -0.46010037535085324 \t\n",
      "average logL for iteration 1500: -0.46010037535085324 \t\n",
      "average logL for iteration 2000: -0.46010037535085324 \t\n",
      "average logL for iteration 2500: -0.46010037535085324 \t\n",
      "average logL for iteration 3000: -0.46010037535085324 \t\n",
      "average logL for iteration 3500: -0.46010037535085324 \t\n",
      "average logL for iteration 4000: -0.46010037535085324 \t\n",
      "average logL for iteration 4500: -0.46010037535085324 \t\n",
      "average logL for iteration 5000: -0.46010037535085324 \t\n",
      "average logL for iteration 5500: -0.46010037535085324 \t\n",
      "average logL for iteration 6000: -0.46010037535085324 \t\n",
      "average logL for iteration 6500: -0.46010037535085324 \t\n",
      "average logL for iteration 7000: -0.46010037535085324 \t\n",
      "average logL for iteration 7500: -0.46010037535085324 \t\n",
      "average logL for iteration 8000: -0.46010037535085324 \t\n",
      "average logL for iteration 8500: -0.46010037535085324 \t\n",
      "average logL for iteration 9000: -0.46010037535085324 \t\n",
      "average logL for iteration 9500: -0.46010037535085324 \t\n",
      "Training avgLogL:  -0.46010037535085324\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "\"\"\"add import here so can just run this cell alone when update the source code\"\"\"\n",
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "training_accuracy, testing_accuracy = train_and_predict_logistic(lm, beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Regularization. Similar to linear regression, an regularization term could be added to logistic regression. \n",
    "The objective function becomes following: \n",
    "    $$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ \\exp\\{ x_i^T \\beta \\} \\right) \\right) + \\lambda \\sum_j \\beta_j^2,$$ \n",
    "where $\\lambda \\leq 0$, which is a hyper parameter that controls the trade off. Take the derivative $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$ of this provided objective function and provide the batch gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> \n",
    "    1. They are the same because both methods achieve the optimum for log likelihood and the result beta is unique. <br>\n",
    "    2. The derivative of new objective function is the derivative of original J + derivate of regularization term. \n",
    "        $$ \\frac{\\partial J(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^Nx_{ij}(y_i-p_i(\\beta)) + 2\\lambda\\beta_j$$ \n",
    "       The batch gradient update is \n",
    "        $$ \\beta^{new} = \\beta^{old} + \\eta v(v_j = \\sum_{i=1}^Nx_{ij}(y_i-p_i(\\beta)) + 2\\lambda\\beta_j)$$\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the same implementation for another small dataset with each datapoint $x$ with only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training avgLogL:  -0.329147431295712\n",
      "[-2.6205116   0.76037154  1.17194674]\n",
      "Training accuracy is:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "train_predict_y = lm.predict(lm.train_x, beta)\n",
    "training_accuracy = lm.compute_accuracy(train_predict_y, lm.train_y)\n",
    "print(beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    Yes it is linear as there are only two variables involved with only first indexes.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "    \n",
    "# plot the decision boundary on top of the scattered points\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "x1_values = [i for i in np.arange(0,9,0.01)]\n",
    "x2_values = []\n",
    "\n",
    "# boundary: x1*beta[1] + x2*beta[2] + beta[0] = 0\n",
    "for x1 in x1_values:\n",
    "    x2 = (-beta[0] - x1*beta[1]) / beta[2]\n",
    "    x2_values.append(x2)\n",
    "\n",
    "plt.plot(x1_values, x2_values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
